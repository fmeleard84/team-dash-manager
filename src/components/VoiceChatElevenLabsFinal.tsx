import { useState, useRef, useEffect } from 'react';
import { Button } from '@/components/ui/button';
import { Badge } from '@/components/ui/badge';
import { Mic, MicOff, Volume2 } from 'lucide-react';
import { toast } from 'sonner';

const ELEVENLABS_API_KEY = 'sk_78684f6ed063bb3803838d5ce932e5c38d0a308e542381ac';
const VOICE_ID = '21m00Tcm4TlvDq8ikWAM'; // Rachel voice

interface VoiceChatElevenLabsFinalProps {
  onTranscription: (text: string) => void;
  botResponse?: string;
  isEnabled: boolean;
  onToggle: () => void;
}

export function VoiceChatElevenLabsFinal({ 
  onTranscription, 
  botResponse, 
  isEnabled, 
  onToggle 
}: VoiceChatElevenLabsFinalProps) {
  const [isListening, setIsListening] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const recognitionRef = useRef<any>(null);
  const audioRef = useRef<HTMLAudioElement | null>(null);

  // Reconnaissance vocale native du navigateur
  useEffect(() => {
    if (!isEnabled) {
      if (recognitionRef.current) {
        recognitionRef.current.stop();
        recognitionRef.current = null;
      }
      return;
    }

    const SpeechRecognition = (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition;
    if (!SpeechRecognition) {
      toast.error('Reconnaissance vocale non support√©e');
      onToggle();
      return;
    }

    const recognition = new SpeechRecognition();
    recognition.continuous = true;
    recognition.interimResults = true; // Activer pour d√©tecter quand l'utilisateur parle
    recognition.lang = 'fr-FR';

    recognition.onstart = () => {
      console.log('üéôÔ∏è √âcoute active');
      setIsListening(true);
    };

    let finalTranscript = '';
    let silenceTimer: NodeJS.Timeout | null = null;
    
    recognition.onresult = (event: any) => {
      // Ne pas traiter si on est en train de parler
      if (audioRef.current || isSpeaking) {
        console.log('üö´ R√©sultat ignor√© - IA en train de parler');
        return;
      }
      
      let interimTranscript = '';
      
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        
        if (event.results[i].isFinal) {
          finalTranscript += transcript + ' ';
          
          // Annuler le timer pr√©c√©dent
          if (silenceTimer) {
            clearTimeout(silenceTimer);
          }
          
          // Attendre un silence de 1.5 secondes avant d'envoyer
          silenceTimer = setTimeout(() => {
            if (finalTranscript.trim()) {
              console.log('üìù Phrase compl√®te:', finalTranscript.trim());
              onTranscription(finalTranscript.trim());
              finalTranscript = '';
            }
          }, 1500); // Attendre 1.5 secondes de silence
          
        } else {
          interimTranscript += transcript;
        }
      }
      
      // Afficher ce qui est en cours de reconnaissance
      if (interimTranscript) {
        console.log('üí≠ En cours:', interimTranscript);
      }
    };

    recognition.onerror = (event: any) => {
      if (event.error === 'not-allowed') {
        toast.error('Microphone non autoris√©');
        onToggle();
      }
    };

    recognition.onend = () => {
      setIsListening(false);
      console.log('üîö Session √©coute termin√©e');
      
      // Red√©marrer seulement si activ√© ET pas en train de parler
      if (isEnabled && !audioRef.current && !isSpeaking) {
        setTimeout(() => {
          if (isEnabled && recognitionRef.current && !audioRef.current) {
            try { 
              recognitionRef.current.start();
              console.log('üîÑ Red√©marrage √©coute');
            } catch(e) {
              console.log('Erreur red√©marrage:', e);
            }
          }
        }, 300); // D√©lai plus long pour √©viter les conflits
      }
    };

    recognitionRef.current = recognition;
    recognition.start();

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop();
        recognitionRef.current = null;
      }
    };
  }, [isEnabled, onTranscription, onToggle]);

  // Synth√®se vocale ElevenLabs
  useEffect(() => {
    if (!botResponse || !isEnabled || audioRef.current) return;

    const speak = async () => {
      try {
        // ARR√äTER COMPL√àTEMENT l'√©coute pendant la parole
        if (recognitionRef.current) {
          try {
            recognitionRef.current.abort(); // Utiliser abort au lieu de stop
            console.log('üîá Micro arr√™t√© pour la synth√®se');
          } catch(e) {
            console.log('Micro d√©j√† arr√™t√©');
          }
        }

        setIsSpeaking(true);
        
        const response = await fetch(
          `https://api.elevenlabs.io/v1/text-to-speech/${VOICE_ID}/stream`,
          {
            method: 'POST',
            headers: {
              'Accept': 'audio/mpeg',
              'Content-Type': 'application/json',
              'xi-api-key': ELEVENLABS_API_KEY,
            },
            body: JSON.stringify({
              text: botResponse.replace(/\*\*/g, '').substring(0, 1000),
              model_id: 'eleven_multilingual_v2',
              voice_settings: {
                stability: 0.5,
                similarity_boost: 0.75
              }
            })
          }
        );

        if (response.ok) {
          const audioBlob = await response.blob();
          const audioUrl = URL.createObjectURL(audioBlob);
          const audio = new Audio(audioUrl);
          audioRef.current = audio;
          
          audio.onended = () => {
            URL.revokeObjectURL(audioUrl);
            audioRef.current = null;
            setIsSpeaking(false);
            console.log('‚úÖ ElevenLabs a fini de parler');
            
            // Red√©marrer l'√©coute APR√àS un d√©lai pour √©viter les conflits
            if (isEnabled && recognitionRef.current) {
              setTimeout(() => {
                try { 
                  recognitionRef.current.start();
                  console.log('üéôÔ∏è Micro r√©activ√©');
                } catch(e) {
                  console.log('Erreur r√©activation micro:', e);
                }
              }, 500); // D√©lai de 500ms pour s'assurer que tout est termin√©
            }
          };
          
          await audio.play();
          console.log('üîä ElevenLabs parle');
        }
      } catch (error) {
        console.error('Erreur TTS:', error);
        setIsSpeaking(false);
      }
    };

    speak();
  }, [botResponse, isEnabled]);

  // Nettoyage
  useEffect(() => {
    return () => {
      if (audioRef.current) {
        audioRef.current.pause();
        audioRef.current = null;
      }
      if (recognitionRef.current) {
        recognitionRef.current.stop();
        recognitionRef.current = null;
      }
    };
  }, []);

  return (
    <div className="flex items-center gap-3">
      <Button
        onClick={onToggle}
        variant={isEnabled ? "default" : "outline"}
        size="sm"
        className={isEnabled ? "bg-gradient-to-r from-purple-600 to-pink-600" : ""}
      >
        {isEnabled ? (
          <>
            <Mic className="w-4 h-4 mr-2 animate-pulse" />
            Mode Vocal ElevenLabs
          </>
        ) : (
          <>
            <MicOff className="w-4 h-4 mr-2" />
            Activer Mode Vocal
          </>
        )}
      </Button>
      
      {isEnabled && (
        <div className="flex items-center gap-2">
          {isSpeaking ? (
            <Badge variant="secondary" className="bg-purple-500 text-white">
              <Volume2 className="w-3 h-3 mr-1 animate-pulse" />
              IA parle
            </Badge>
          ) : isListening ? (
            <Badge variant="secondary" className="bg-green-500 text-white">
              <Mic className="w-3 h-3 mr-1" />
              √âcoute...
            </Badge>
          ) : (
            <Badge variant="outline">
              ‚è∏Ô∏è Pause
            </Badge>
          )}
        </div>
      )}
    </div>
  );
}